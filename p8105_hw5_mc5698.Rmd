---
title: "p8105_hw5_mc5698"
output: github_document 
date: "2024-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rvest)
library(broom)
```
#Problem 1
```{r}
# Run the simulation
birthday_sim = 
  function(n) {
  birthdays <- sample(1:365, n, replace = TRUE)
  return(any(duplicated(birthdays))) 
}
```

```{r}
sim_results_df =
  expand_grid(
  group_size = 2:50,
  iter = 1:10000
) |>
  mutate(
    has_shared_birthday = map_lgl(group_size, birthday_sim)
  )
```

```{r}
# plot
birthday_prob <- sim_results_df %>%
  group_by(group_size) %>%
  summarize(probability = mean(has_shared_birthday))

ggplot(birthday_prob, aes(x = group_size, y = probability)) +
  geom_line() +
  labs(title = "Probability that at least two people share a birthday",
       x = "Group Size",
       y = "Probability of Shared Birthday") +
  theme_minimal()

```
This graph shows the probability that at least two people in a group share the same birthday increases rapidly as the group size grows. About 23 people, thereâ€™s already over a 50% chance of a shared birthday, and when the group reaches 50, the probability is close to 100%.

#Problem 2
```{r}
# Simulation parameters
n <- 30
sigma <- 5
mu_values <- 0:6
n_simulations <- 5000
alpha <- 0.05
```

```{r}
#Define function
one_sample_sim = function(mu) {
  x <- rnorm(n, mean = mu, sd = sigma)
  t_test_result <- t.test(x, mu = 0) %>% 
    tidy() %>% 
    select(estimate, p.value)  
  colnames(t_test_result) <- c("mu_hat", "p_value")
  return(t_test_result)
}
```

```{r}
#Run the simulation
set.seed(123) 
sim_results_df <- expand_grid(
  mu = 0:6,
  iter = 1:5000
) %>%
  mutate(
    test_results = map(mu, one_sample_sim)
  ) %>%
  unnest(test_results)

```

```{r}
power_analysis = sim_results_df %>%
  group_by(mu) %>%
  summarize(
    power = mean(p_value < alpha),
    avg_mu_hat = mean(mu_hat),             
    avg_mu_hat_rejected = mean(mu_hat[p_value < alpha], na.rm = TRUE))
```

```{r}
# Plot power as a function of mu
ggplot(power_analysis, aes(x = mu, y = power)) +
  geom_line() +
  labs(title = "Power of One Sample t-Test by Effect Size",
       x = "True Mean (mu)",
       y = "Power") +
  theme_minimal()

```
This graph shows that as the effect size (true mean) increases, the power of the one-sample t-test also increases. When Î¼ is close to zero, the power is low, meaning the test rarely detects a difference. As true mean increases, the power quickly rises, reaching near certainty around mu = 4 which shows that larger effect sizes make it more likely for the test to correctly reject the null hypothesis.
```{r}
# Plot the average estimate of ðœ‡Ì‚ on the y axis and the true value of ðœ‡ on the x axis.
ggplot(power_analysis, aes(x = mu)) +
  geom_line(aes(y = avg_mu_hat, color = "All Samples")) +
  geom_line(aes(y = avg_mu_hat_rejected, color = "Rejected Samples")) +
  labs(title = "Average Estimate of mu_hat by Effect Size",
       x = "True Mean (mu)",
       y = "Average Estimate of mu_hat") +
  scale_color_manual(name = "Legend", values = c("All Samples" = "green", "Rejected Samples" = "blue")) +
  theme_minimal()
```
The graph shows that the average estimate of mu_hat is higher in samples where the null was rejected (blue line) compared to all samples (green line) as mu increases. This occurs because samples with higher means are more likely to reject the null, creating a positive bias in the rejected samples. Therefore, the average estimate in rejected samples does not equal the true mean.

#Problem 3
```{r}
#Load dataset
homicides <- read_csv("data/homicide-data.csv")
homicides
summary(homicides)
```
The raw dataset has 52,179 records from U.S. cities, detailing individual homicides from January 1, 2007, to November 5, 2015. Each record includes the unique identifier (uid), report date, victim details (name, race, age, and gender), and the city and state. Moreover, latitude and longitude coordinates provide precise location data. 
```{r}
#Define city_state variable
unsolved_cases <- c("Closed without arrest", "Open/No arrest")

homicides <- homicides %>%
  mutate(city_state = paste(city, state, sep = ", ")) %>%  
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% unsolved_cases)
  ) %>%
  ungroup()
homicides
```

```{r}
#Use the prop.test function to estimate the proportion of homicides that are unsolved
baltimore_data <- homicides %>%
  filter(city_state == "Baltimore, MD")

baltimore_test = prop.test(baltimore_data$unsolved_homicides,
                           baltimore_data$total_homicides)
baltimore_results = 
  tidy(baltimore_test) %>%
  select(estimate, conf.low, conf.high) %>%
  rename(
    proportion_unsolved = estimate,
    conf_low = conf.low,
    conf_high = conf.high
  )
baltimore_results
```
```{r}
#Run prop.test for each city
city_proportions <- homicides %>%
  mutate(
    test = map2(unsolved_homicides, total_homicides, ~ binom.test(.x, .y)),
    test_summary = map(test, broom::tidy)
  ) %>%
  unnest(test_summary) %>%
  select(city_state, estimate, conf.low, conf.high)
city_proportions
```

```{r}
# Order cities by proportion
city_proportions <- city_proportions %>%
  arrange(desc(estimate))

# Plotting
ggplot(city_proportions, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  theme_minimal() +
  coord_flip()
```
The graph shows the proportion of unsolved homicides across U.S. cities, with error bars indicating confidence intervals. There is wide variation, with some cities below 25% unsolved and others nearing 75% which highlights disparities in case resolution rates.
